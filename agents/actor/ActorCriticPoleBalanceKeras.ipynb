{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ActorCriticPoleBalanceKeras.ipynb","version":"0.3.2","provenance":[{"file_id":"14_ZU6q26jM8i16utqjHvy9b22PmWsidk","timestamp":1543254254104}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"PXoDFaubpB0p","colab_type":"text"},"cell_type":"markdown","source":["# Solving CartPole using the Actor Critic Model"]},{"metadata":{"id":"qJ_KSZ1RzbVO","colab_type":"text"},"cell_type":"markdown","source":["Let's start by installing OpenAI Gym in order to get the CartPole environment"]},{"metadata":{"id":"W07n66lhB6yq","colab_type":"code","outputId":"f4f22003-2bf4-44c1-92ef-1948a04f63fe","executionInfo":{"status":"ok","timestamp":1544643890118,"user_tz":300,"elapsed":5402,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":210}},"cell_type":"code","source":["!pip install gym"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"],"name":"stdout"}]},{"metadata":{"id":"xEaZoFujzfu6","colab_type":"text"},"cell_type":"markdown","source":["Now to load all the packages we need"]},{"metadata":{"id":"l4X6w1HH_aYo","colab_type":"code","outputId":"ed753187-45bf-43a9-d5da-b5b63e3719c1","executionInfo":{"status":"ok","timestamp":1544643891331,"user_tz":300,"elapsed":6596,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import gym\n","import numpy as np\n","import random\n","from collections import deque\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"BVpl8t-YCM3D","colab_type":"text"},"cell_type":"markdown","source":["## Actor Critic Agent"]},{"metadata":{"id":"2Ar3yLxrtU7P","colab_type":"text"},"cell_type":"markdown","source":["The CriticAcorAgent class is going to house all the logic necessary to have a Deep Actor-Critic Agent. Since there's a lot going on here, this section will be longer than the others."]},{"metadata":{"id":"FNyEJDty0Vn_","colab_type":"text"},"cell_type":"markdown","source":["### Parameters"]},{"metadata":{"id":"kGIzk0E00XEh","colab_type":"text"},"cell_type":"markdown","source":["There are several parameters that are hard-coded into the model that should be tweaked when applying it to different problems to see if it affects performance. We will describe each parameter briefly here.\n","\n","\n","\n","* Epsilon: The exploration rate. How often will the agent choose a random move during training instead of relying on information it already has. Helps the agent go down paths it normally wouldn't in hopes for higher long term rewards.\n","  - Epsilon Decay: How much our epsilon decreases after each update\n","  - Epsilon Min: The lowest rate of exploration we'll allow during training\n","* Gamma: Discount rate. This tells us how much we prioritize current versus future rewards.\n","* Tau: Affects how much we shift our knowledge based off of new information.\n","* Learning Rate: Affects the optimization procedures in the neural networks.\n","\n"]},{"metadata":{"id":"0TOKiRNm13E3","colab_type":"text"},"cell_type":"markdown","source":["### Fixed Q-Targets"]},{"metadata":{"id":"jGcn7vPp15VD","colab_type":"text"},"cell_type":"markdown","source":["In Q-Learning we update our Q_Table through the following function:\n","\n","$Q_{TableEntry}(state, action) = Reward + max(Q_{TableEntry}(state))$\n","\n","Since our update is dependent on the same table itself, we can start to get correlated entries. This could cause oscillations to happen in our training. \n","\n","To combat this, we implemented a target model. It essentially is a copy of the original model, except that the values do not update as rapidly. The rate at which the target model updates is dependent upon `Tau` in our parameter list."]},{"metadata":{"id":"iqJQvpl94h5_","colab_type":"text"},"cell_type":"markdown","source":["### Agent Workflow"]},{"metadata":{"id":"IGHu86gMrWB7","colab_type":"text"},"cell_type":"markdown","source":["1. Perform actions and record the results in the agent's memory\n","2. After every action, perform what's called a replay and sample a random $batchSize$ memories to train on.\n","3. During training on each experience do the following\n","  - For the critic, update the value of the current state by taking the reward and adding the discounted value of the next state as determined by the critic.\n","  - For the actor, get the action values for the current state predicted by the actor, and update it with the action taken and the value of the next state as determined by the critic.\n"]},{"metadata":{"id":"yHQQa4PKCKtO","colab_type":"code","colab":{}},"cell_type":"code","source":["class CriticActorAgent:\n","  def __init__(self, state_size, action_size):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    # The deque will only contain the last 2000 entries\n","    self.memory = deque(maxlen=2000)\n","    self.gamma = 0.95 # Discount Rate\n","    self.epsilon = 1.0 # Exploration Rate\n","    self.epsilon_min = 0.001\n","    self.epsilon_decay = 0.995\n","    self.learning_rate = 0.001 # For the neural net optimizer\n","    self.critic_model = self._build_critic_model()\n","    self.actor_model = self._build_actor_model()\n","    # Semi-Fixed Q-Targets \n","    self.target_critic_model = self._build_critic_model()\n","    self.target_actor_model = self._build_actor_model()\n","    self.tau = 0.1 # Update the target model by 10% each iteration\n","    \n","    \n","  # What is the value for any given state?\n","  def _build_critic_model(self):\n","    model = Sequential()\n","    model.add(Dense(self.state_size, activation='relu'))\n","    model.add(Dense(20, activation='relu'))\n","    model.add(Dense(1, activation='linear'))\n","    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","    return model\n","    \n","  # What are the action values for the possible actions in a given state?\n","  def _build_actor_model(self):\n","    model = Sequential()\n","    model.add(Dense(self.state_size, activation='relu'))\n","    model.add(Dense(20, activation='relu'))\n","    model.add(Dense(self.action_size, activation='linear'))\n","    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","    return model\n","  \n","  def update_target_model(self, model, target_model):\n","    for layer_target, layer_src in zip(target_model.layers, model.layers):\n","      weights = layer_src.get_weights()\n","      target_weights = layer_target.get_weights()\n","      \n","      # Adjust the weights of the target to be tau proportion closer to the current\n","      for i in range(len(weights)):\n","            target_weights[i] = self.tau * weights[i] + (1 - self.tau) * target_weights[i]\n","      \n","      layer_target.set_weights(target_weights)\n","      \n","  def update_actor_target(self):\n","    self.update_target_model(self.actor_model, self.target_actor_model)\n","    \n","  def update_critic_target(self):\n","    self.update_target_model(self.critic_model, self.target_critic_model)\n","  \n","  def remember(self, state, action, reward, next_state, done):\n","    self.memory.append((state, action, reward, next_state, done))\n","    \n","  def act_random(self):\n","    return random.randrange(self.action_size)\n","  \n","  def best_act(self, state):\n","    # Choose the best action based on what we already know\n","    # If all the action values for a given state is the same, then act randomly\n","    action_values = self.target_actor_model.predict(state)[0]\n","    action = self.act_random() if np.all(action_values[0] == action_values) else np.argmax(action_values)\n","    return action\n","    \n","  def act(self, state):\n","    action = self.act_random() if np.random.rand() <= self.epsilon else self.best_act(state)\n","    if self.epsilon > self.epsilon_min:\n","      self.epsilon *= self.epsilon_decay\n","    return action\n","  \n","  \n","  def replay(self, batch_size):\n","    minibatch = random.sample(self.memory, batch_size)\n","    self._train_critic(minibatch)\n","    self._train_actor(minibatch)\n","  \n","  # Calculate the value for the current state by taking the reward and discounted future rewards\n","  # and fit the calculation into the critic model.\n","  # This effectively updates what the value for the current state is.\n","  # Think about how this fits with the whole max(x) policy deal\n","  def _train_critic(self, minibatch):\n","    for state, action, reward, next_state, done in minibatch:\n","      target = reward\n","      if not done:\n","        future_value = self.target_critic_model.predict(next_state)\n","        target = reward + self.gamma * future_value\n","      target = np.array(target)\n","      target = np.reshape(target, [1, 1])\n","      self.critic_model.fit(state, target, epochs = 1, verbose = 0)\n","    self.update_critic_target()\n","    \n","  \n","  # Grab the action values for the current state and update the one for the action taken\n","  # with the reward and the discounted future value predicted from the critic\n","  def _train_actor(self, minibatch):\n","    for state, action, reward, next_state, done in minibatch:\n","      action_values = self.target_actor_model.predict(state)\n","      target = reward\n","      if not done:\n","        next_state_value = self.target_critic_model.predict(next_state)\n","        target = reward + self.gamma * next_state_value\n","      action_values[0][action] = target\n","      self.actor_model.fit(state, action_values, epochs = 1, verbose = 0)\n","    self.update_actor_target()\n","  \n","  def load(self, name):\n","    self.critic_model.load_weights(name)\n","    \n","  def save(self, name):\n","    self.critic_model.save_weights(name)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"7xCFCfmapqdT"},"cell_type":"markdown","source":["## Training"]},{"metadata":{"id":"KTQprVosBZ-q","colab_type":"text"},"cell_type":"markdown","source":["We will now use our Actor-Critic Agent to train it in CartPole by simulating a lot of runs through the environment."]},{"metadata":{"id":"IB10AdjtEPGe","colab_type":"code","outputId":"4a8ecb16-32cf-4d13-ecf6-a0d157287cd2","executionInfo":{"status":"ok","timestamp":1544644933912,"user_tz":300,"elapsed":870906,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":600}},"cell_type":"code","source":["env = gym.make('CartPole-v1')\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","batch_size = 32\n","\n","agent = CriticActorAgent(state_size, action_size)\n","EPISODES = 30\n","for episode_num in range(1, EPISODES + 1):\n","  state = env.reset()\n","  state = np.reshape(state, [1, state_size])\n","  \n","  score = 0\n","  done = False\n","  while not done:\n","    action = agent.act(state)\n","    next_state, reward, done, _ = env.step(action)\n","    \n","    # If the episode is over that means you failed :(\n","    reward = reward if not done else -10\n","    \n","    next_state = np.reshape(next_state, [1, state_size])\n","    \n","    agent.remember(state, action, reward, next_state, done)\n","    \n","    state = next_state\n","    \n","    # Replay less?\n","    if len(agent.memory) > batch_size:\n","      agent.replay(batch_size)\n","    \n","    if done:\n","      print(\"episode: {}/{}, score: {}, epsilon: {:.2}\"\n","          .format(episode_num, EPISODES, score, agent.epsilon))\n","    \n","    # Made it to the next frame :)\n","    score += 1\n","  \n","  # Save model every 100 episodes\n","  if episode_num % 100 == 0:\n","    print(\"SAVING CURRENT MODEL\")\n","    agent.save(SAVE_DIR + str(episode_num))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"],"name":"stderr"},{"output_type":"stream","text":["episode: 1/30, score: 14, epsilon: 0.93\n","episode: 2/30, score: 12, epsilon: 0.87\n","episode: 3/30, score: 19, epsilon: 0.79\n","episode: 4/30, score: 46, epsilon: 0.62\n","episode: 5/30, score: 9, epsilon: 0.59\n","episode: 6/30, score: 10, epsilon: 0.56\n","episode: 7/30, score: 41, epsilon: 0.45\n","episode: 8/30, score: 63, epsilon: 0.33\n","episode: 9/30, score: 18, epsilon: 0.3\n","episode: 10/30, score: 24, epsilon: 0.26\n","episode: 11/30, score: 38, epsilon: 0.22\n","episode: 12/30, score: 31, epsilon: 0.18\n","episode: 13/30, score: 30, epsilon: 0.16\n","episode: 14/30, score: 50, epsilon: 0.12\n","episode: 15/30, score: 125, epsilon: 0.065\n","episode: 16/30, score: 119, epsilon: 0.036\n","episode: 17/30, score: 133, epsilon: 0.018\n","episode: 18/30, score: 109, epsilon: 0.01\n","episode: 19/30, score: 114, epsilon: 0.0059\n","episode: 20/30, score: 120, epsilon: 0.0032\n","episode: 21/30, score: 118, epsilon: 0.0018\n","episode: 22/30, score: 124, epsilon: 0.001\n","episode: 23/30, score: 137, epsilon: 0.001\n","episode: 24/30, score: 94, epsilon: 0.001\n","episode: 25/30, score: 98, epsilon: 0.001\n","episode: 26/30, score: 101, epsilon: 0.001\n","episode: 27/30, score: 121, epsilon: 0.001\n","episode: 28/30, score: 99, epsilon: 0.001\n","episode: 29/30, score: 96, epsilon: 0.001\n","episode: 30/30, score: 140, epsilon: 0.001\n"],"name":"stdout"}]},{"metadata":{"id":"DnU8WAwz49-o","colab_type":"text"},"cell_type":"markdown","source":["## Evaluate Performance"]},{"metadata":{"id":"3JHXjw1YBjLX","colab_type":"text"},"cell_type":"markdown","source":["Now to test how our agent performed, we will run through more scenarios except this time we don't allow the agent to choose randomly and have it rely on its previous experiences."]},{"metadata":{"id":"7HemRoBkGTEJ","colab_type":"code","outputId":"8be76c98-e361-4dd2-bf95-f1856e45cc7d","executionInfo":{"status":"ok","timestamp":1544645609840,"user_tz":300,"elapsed":127426,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":1777}},"cell_type":"code","source":["## Show performance\n","trials = 1000\n","agent.epsilon = 0\n","scores_list = []\n","for episode_num in range(1, trials + 1):\n","  state = env.reset()\n","  state = np.reshape(state, [1, state_size])\n","  \n","  score = 0\n","  done = False\n","  while not done:\n","    action = agent.act(state)\n","    next_state, reward, done, _ = env.step(action)\n","    \n","    # If the episode is over that means you failed :(\n","    reward = reward if not done else -10\n","    \n","    next_state = np.reshape(next_state, [1, state_size])\n","    state = next_state\n","    \n","    if done and (episode_num % 10 == 0):\n","      print(\"episode: {}/{}, score: {}\"\n","          .format(episode_num, trials, score))\n","    \n","    # Made it to the next frame :)\n","    score += 1\n","  \n","  scores_list.append(score)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["episode: 10/1000, score: 117\n","episode: 20/1000, score: 120\n","episode: 30/1000, score: 106\n","episode: 40/1000, score: 120\n","episode: 50/1000, score: 159\n","episode: 60/1000, score: 142\n","episode: 70/1000, score: 97\n","episode: 80/1000, score: 140\n","episode: 90/1000, score: 142\n","episode: 100/1000, score: 112\n","episode: 110/1000, score: 123\n","episode: 120/1000, score: 94\n","episode: 130/1000, score: 117\n","episode: 140/1000, score: 124\n","episode: 150/1000, score: 103\n","episode: 160/1000, score: 158\n","episode: 170/1000, score: 114\n","episode: 180/1000, score: 165\n","episode: 190/1000, score: 115\n","episode: 200/1000, score: 117\n","episode: 210/1000, score: 272\n","episode: 220/1000, score: 262\n","episode: 230/1000, score: 113\n","episode: 240/1000, score: 103\n","episode: 250/1000, score: 201\n","episode: 260/1000, score: 140\n","episode: 270/1000, score: 112\n","episode: 280/1000, score: 133\n","episode: 290/1000, score: 102\n","episode: 300/1000, score: 113\n","episode: 310/1000, score: 150\n","episode: 320/1000, score: 95\n","episode: 330/1000, score: 124\n","episode: 340/1000, score: 113\n","episode: 350/1000, score: 118\n","episode: 360/1000, score: 110\n","episode: 370/1000, score: 95\n","episode: 380/1000, score: 104\n","episode: 390/1000, score: 181\n","episode: 400/1000, score: 105\n","episode: 410/1000, score: 108\n","episode: 420/1000, score: 129\n","episode: 430/1000, score: 110\n","episode: 440/1000, score: 110\n","episode: 450/1000, score: 208\n","episode: 460/1000, score: 102\n","episode: 470/1000, score: 134\n","episode: 480/1000, score: 135\n","episode: 490/1000, score: 112\n","episode: 500/1000, score: 120\n","episode: 510/1000, score: 103\n","episode: 520/1000, score: 113\n","episode: 530/1000, score: 208\n","episode: 540/1000, score: 112\n","episode: 550/1000, score: 103\n","episode: 560/1000, score: 96\n","episode: 570/1000, score: 98\n","episode: 580/1000, score: 99\n","episode: 590/1000, score: 114\n","episode: 600/1000, score: 166\n","episode: 610/1000, score: 94\n","episode: 620/1000, score: 98\n","episode: 630/1000, score: 218\n","episode: 640/1000, score: 116\n","episode: 650/1000, score: 100\n","episode: 660/1000, score: 101\n","episode: 670/1000, score: 92\n","episode: 680/1000, score: 123\n","episode: 690/1000, score: 174\n","episode: 700/1000, score: 121\n","episode: 710/1000, score: 146\n","episode: 720/1000, score: 326\n","episode: 730/1000, score: 171\n","episode: 740/1000, score: 107\n","episode: 750/1000, score: 128\n","episode: 760/1000, score: 155\n","episode: 770/1000, score: 100\n","episode: 780/1000, score: 127\n","episode: 790/1000, score: 123\n","episode: 800/1000, score: 134\n","episode: 810/1000, score: 127\n","episode: 820/1000, score: 122\n","episode: 830/1000, score: 109\n","episode: 840/1000, score: 107\n","episode: 850/1000, score: 153\n","episode: 860/1000, score: 97\n","episode: 870/1000, score: 160\n","episode: 880/1000, score: 97\n","episode: 890/1000, score: 124\n","episode: 900/1000, score: 119\n","episode: 910/1000, score: 108\n","episode: 920/1000, score: 100\n","episode: 930/1000, score: 113\n","episode: 940/1000, score: 93\n","episode: 950/1000, score: 130\n","episode: 960/1000, score: 142\n","episode: 970/1000, score: 132\n","episode: 980/1000, score: 98\n","episode: 990/1000, score: 123\n","episode: 1000/1000, score: 167\n"],"name":"stdout"}]},{"metadata":{"id":"FvVXCyj3CC1D","colab_type":"text"},"cell_type":"markdown","source":["## Analysis of Performance"]},{"metadata":{"id":"Zu_tyQ5RCFCD","colab_type":"text"},"cell_type":"markdown","source":["Let us load the matplotlib library to have some visualizations\n"]},{"metadata":{"id":"bp50H1vOIVMM","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AK7iJ2Z9CeMe","colab_type":"code","colab":{}},"cell_type":"code","source":["scores = np.array(scores_list)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F7p6kGlSIT_r","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"Mean Score: {}, Standard Deviation of Scores {}\".format(scores.mean(), scores.std()))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eimAzjbQJNA5","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.hist(scores)"],"execution_count":0,"outputs":[]}]}