{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Taxi Reinforcement Learning.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"XexRbrYEr0Tr","colab_type":"text"},"cell_type":"markdown","source":["# Solving Taxicab with Q-Learning"]},{"metadata":{"id":"DnAEJg-us-aR","colab_type":"text"},"cell_type":"markdown","source":["Install the OpenAI gym package to get the Taxicab environment"]},{"metadata":{"id":"ghZqiQrYJrQj","colab_type":"code","outputId":"3d0ad51d-ac3b-4a5d-d3b0-3e9b993d2bb1","executionInfo":{"status":"ok","timestamp":1544643715303,"user_tz":300,"elapsed":3618,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":210}},"cell_type":"code","source":["!pip install gym"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"],"name":"stdout"}]},{"metadata":{"id":"HZs11OeBtCls","colab_type":"text"},"cell_type":"markdown","source":["Import the following packages as wel'll need it later"]},{"metadata":{"id":"TWZYesyhKGyr","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import numpy as np\n","import random\n","# For the plotting\n","from IPython.display import clear_output\n","from time import sleep"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HCvsg13dtGEt","colab_type":"text"},"cell_type":"markdown","source":["Load up the Blackjack Environment"]},{"metadata":{"id":"Q7Pyys31KNVE","colab_type":"code","outputId":"9c4f69a4-2370-40ba-a874-abf82b9a27b5","executionInfo":{"status":"ok","timestamp":1544643715308,"user_tz":300,"elapsed":3601,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["env = gym.make(\"Taxi-v2\").env"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"],"name":"stderr"}]},{"metadata":{"id":"_KoesXc6tMdB","colab_type":"text"},"cell_type":"markdown","source":["For the curious, we'll display the state and action space size for the game"]},{"metadata":{"id":"fLgr9L6CKYWL","colab_type":"code","outputId":"274dcf0c-0cc2-4b3f-eb66-c41689a20449","executionInfo":{"status":"ok","timestamp":1544643715309,"user_tz":300,"elapsed":3578,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Action Space Discrete(6)\n","State Space Discrete(500)\n"],"name":"stdout"}]},{"metadata":{"id":"CrT7_vvgJj4e","colab_type":"text"},"cell_type":"markdown","source":["This is the graphical representation of the game\n","  - The yellow highlight is the taxi\n","   - The `R` is the pickup point\n","   - The `B` is the dropoff point\n","   `Y` and `G` are other points the taxi can pick people up and drop them off"]},{"metadata":{"id":"OopZ1NqgrqD0","colab_type":"code","outputId":"58e5bbf3-0d16-4cf2-eebe-0bc27bbe35f7","executionInfo":{"status":"ok","timestamp":1544643715310,"user_tz":300,"elapsed":3565,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"cell_type":"code","source":["env.render()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : : : : |\n","|\u001b[43m \u001b[0m: : : : |\n","| | : | : |\n","|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","\n"],"name":"stdout"}]},{"metadata":{"id":"VMr7pphvtSVH","colab_type":"text"},"cell_type":"markdown","source":["## Q Learning Agent"]},{"metadata":{"id":"2Ar3yLxrtU7P","colab_type":"text"},"cell_type":"markdown","source":["The QAgent class is going to house all the logic necessary to have a Q-Learning Agent. Since there's a lot going on here, this section will be longer than the others."]},{"metadata":{"id":"FNyEJDty0Vn_","colab_type":"text"},"cell_type":"markdown","source":["### Parameters"]},{"metadata":{"id":"kGIzk0E00XEh","colab_type":"text"},"cell_type":"markdown","source":["There are several parameters that are hard-coded into the model that should be tweaked when applying it to different problems to see if it affects performance. We will describe each parameter briefly here.\n","\n","\n","\n","* Epsilon: The exploration rate. How often will the agent choose a random move during training instead of relying on information it already has. Helps the agent go down paths it normally wouldn't in hopes for higher long term rewards.\n","  - Epsilon Decay: How much our epsilon decreases after each update\n","  - Epsilon Min: The lowest rate of exploration we'll allow during training\n","* Gamma: Discount rate. This tells us how much we prioritize current versus future rewards.\n","* Alpha: Affects how much we shift our knowledge based off of new information.\n","\n"]},{"metadata":{"id":"0TOKiRNm13E3","colab_type":"text"},"cell_type":"markdown","source":["### Fixed Q-Targets"]},{"metadata":{"id":"jGcn7vPp15VD","colab_type":"text"},"cell_type":"markdown","source":["In Q-Learning we update our Q_Table through the following function:\n","\n","$Q_{TableEntry}(state, action) = Reward + max(Q_{TableEntry}(state))$\n","\n","Since our update is dependent on the same table itself, we can start to get correlated entries. This could cause oscillations to happen in our training. \n","\n","To combat this, we implemented a target model. It essentially is a copy of the original model, except that the values do not update as rapidly. The rate at which the target model updates is dependent upon `Alpha` in our parameter list."]},{"metadata":{"id":"iqJQvpl94h5_","colab_type":"text"},"cell_type":"markdown","source":["### Agent Workflow"]},{"metadata":{"id":"HFW8vNbp5f-V","colab_type":"text"},"cell_type":"markdown","source":["1. Create an empty q-table for both the model and target model.\n","2. Given a starting state, perform an action.\n","3. Once you performed the action on the environment, update the model with information you have gained through the environment: reward, next state, if the environment is finished, etc.\n","4. Then calculate the value of the state.\n","  - If the game is finished, then the reward is the value of that state.\n","  - If not, then take the current reward and add the discounted reward of future states.\n","5. Update the model with the new value.\n","6. Decay the epsilon value as described in the parameters.\n","7. Gradually update the target model.\n","8. Perform an action for the new state, either randomly through epsilon, or by choosing the best action based on what we currently know.\n","9. Repeat steps 3-8."]},{"metadata":{"id":"lBz-t2khgRrs","colab_type":"code","colab":{}},"cell_type":"code","source":["class QAgent:\n","  def __init__(self, state_size, action_size):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.gamma = 0.6 # Discount Rate\n","    self.epsilon = 0.1 # Exploration Rate\n","    self.epsilon_min = 0.001\n","    self.epsilon_decay = 0.9995\n","    self.model = self._build_model()\n","    ## Additional components for Fixed Q-Targets\n","    self.target_model = self._build_model()\n","    # Update the target model by 10% each iteration\n","    self.alpha = 0.1\n","    \n","  def _build_model(self):\n","    # Assumes both self.state_size and self.action_size are lists\n","    model = np.zeros(self.state_size + self.action_size)\n","    return model\n","  \n","  def update_target_model(self):\n","    self.target_model = (1 - self.alpha) * self.target_model + self.alpha * self.model\n","    \n","  def act_random(self):\n","    return random.randrange(self.action_size[0])\n","  \n","  def best_act(self, state):\n","    # Choose the best action based on what we know\n","    # If all the action values are the same, then choose randomly\n","    action = self.act_random() if np.all(self.target_model[state, 0] == self.target_model[state]) else np.argmax(self.target_model[state])\n","    return action\n","  \n","  def act(self, state):\n","    # Act randomly epsilon percent of time, otherwise act greedily\n","    action = self.act_random() if np.random.rand() <= self.epsilon else self.best_act(state)\n","    return action\n","  \n","  def update(self, state, action, reward, next_state, done):\n","    target = reward\n","    if not done:\n","      target = reward + self.gamma * np.amax(self.target_model[next_state])\n","    self.model[state,action] = target\n","    self.update_target_model()\n","    if self.epsilon > self.epsilon_min:\n","      self.epsilon *= self.epsilon_decay\n","      \n","  \n","  def load(self, name):\n","    self.model.load_weights(name)\n","    \n","  def save(self, name):\n","    self.model.save_weights(name)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cFG8QUy0ta21","colab_type":"text"},"cell_type":"markdown","source":["## Training"]},{"metadata":{"id":"rMTb3bnLtc20","colab_type":"text"},"cell_type":"markdown","source":["Now that we have defined our agent, let us train it through playing a lot of games. By the end of it, we would hope that the agent has been through a variety of situations and have learned the best way to combat each one."]},{"metadata":{"id":"Ypz6n0kPrqES","colab_type":"code","outputId":"93e116d9-5a9c-4118-cdab-2b5be5e38fd8","executionInfo":{"status":"ok","timestamp":1544643774302,"user_tz":300,"elapsed":8,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"cell_type":"code","source":["%%time\n","\"\"\"Training the agent\"\"\"\n","state_size =  [ env.observation_space.n ]\n","action_size = [ env.action_space.n ]\n","agent = QAgent(state_size, action_size)\n","EPISODES = 100001\n","for i in range(1, EPISODES):\n","  state = env.reset()\n","\n","  done = False\n","  while not done:\n","    action = agent.act(state)\n","    next_state, reward, done, info = env.step(action) \n","    agent.update(state, action, reward, next_state, done)\n","    state = next_state\n","       \n","  if i % 100 == 0:\n","    clear_output(wait=True)\n","    print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Episode: 100000\n","Training finished.\n","\n","CPU times: user 58.1 s, sys: 1.25 s, total: 59.3 s\n","Wall time: 59 s\n"],"name":"stdout"}]},{"metadata":{"id":"BbU--I3IrqEU","colab_type":"text"},"cell_type":"markdown","source":["## Evaluate Performance of agent"]},{"metadata":{"id":"JWI7dw4TrqED","colab_type":"text"},"cell_type":"markdown","source":["The code below animates the attempts"]},{"metadata":{"id":"AdD3ZUWqrqEE","colab_type":"code","colab":{}},"cell_type":"code","source":["def print_frames(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'].getvalue())\n","        print(f\"Timestep: {i + 1}\")\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        sleep(.2)\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"HpuNQMJ-LKAB","colab_type":"text"},"cell_type":"markdown","source":["Now onto the simulated trials."]},{"metadata":{"id":"Ae5XHd4yrqEW","colab_type":"code","outputId":"57c1e132-a2be-4005-bc24-497aaeb34008","executionInfo":{"status":"ok","timestamp":1544643829363,"user_tz":300,"elapsed":50223,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 100\n","frames = []\n","\n","for i in range(episodes):\n","    state = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","    \n","    done = False\n","    while not done:\n","      action = agent.best_act(state)\n","      state, reward, done, info = env.step(action)\n","      if reward == -10:\n","        penalties += 1\n","\n","      # For the last animation, put each rendered frame\n","      # into frames for future animation\n","      if i == episodes - 1:\n","        frames.append({\n","          'frame': env.render(mode='ansi'),\n","          'state': state,\n","          'action': action,\n","          'reward': reward\n","        })\n","        \n","      epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Results after 100 episodes:\n","Average timesteps per episode: 12.32\n","Average penalties per episode: 0.0\n"],"name":"stdout"}]},{"metadata":{"id":"0R7hAvDsrqEa","colab_type":"code","outputId":"b3f4afaf-0943-4084-c303-50b222882533","executionInfo":{"status":"ok","timestamp":1544643777246,"user_tz":300,"elapsed":125,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":246}},"cell_type":"code","source":["print_frames(frames)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : : : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n","+---------+\n","  (Dropoff)\n","\n","Timestep: 14\n","State: 479\n","Action: 5\n","Reward: 20\n"],"name":"stdout"}]},{"metadata":{"id":"HsQ3n3hHrqD8","colab_type":"text"},"cell_type":"markdown","source":["## Brute Force Solution"]},{"metadata":{"id":"8ODZM_16rqD9","colab_type":"text"},"cell_type":"markdown","source":["Let us compare our previous solution to how a brute force one would do. The code below will randomly choose actions until the task if finished"]},{"metadata":{"id":"CLKqXbedrqD-","colab_type":"code","outputId":"20f33d9f-43ec-4626-e06d-8d628a2244df","executionInfo":{"status":"ok","timestamp":1544643777248,"user_tz":300,"elapsed":65437,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["env.s = 328  # set environment to illustration's state\n","\n","epochs = 0\n","penalties, reward = 0, 0\n","\n","frames = [] # for animation\n","\n","done = False\n","\n","while not done:\n","    action = env.action_space.sample()\n","    state, reward, done, info = env.step(action)\n","\n","    if reward == -10:\n","        penalties += 1\n","    \n","    # Put each rendered frame into dict for animation\n","    frames.append({\n","        'frame': env.render(mode='ansi'),\n","        'state': state,\n","        'action': action,\n","        'reward': reward\n","        }\n","    )\n","\n","    epochs += 1\n","    \n","    \n","print(\"Timesteps taken: {}\".format(epochs))\n","print(\"Penalties incurred: {}\".format(penalties))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Timesteps taken: 256\n","Penalties incurred: 58\n"],"name":"stdout"}]},{"metadata":{"id":"NQwS38_MK863","colab_type":"code","outputId":"a1090f87-ff69-400d-898c-4bd81cfc5825","executionInfo":{"status":"ok","timestamp":1544643829127,"user_tz":300,"elapsed":8,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":246}},"cell_type":"code","source":["print_frames(frames)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["+---------+\n","|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n","| : : : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |B: |\n","+---------+\n","  (Dropoff)\n","\n","Timestep: 256\n","State: 16\n","Action: 5\n","Reward: 20\n"],"name":"stdout"}]},{"metadata":{"id":"Vu2NMZBHNS0a","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}