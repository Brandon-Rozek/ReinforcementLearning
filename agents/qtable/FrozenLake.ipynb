{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FrozenLake.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"XexRbrYEr0Tr","colab_type":"text"},"cell_type":"markdown","source":["# Solving FrozenLake with Q-Learning"]},{"metadata":{"id":"DnAEJg-us-aR","colab_type":"text"},"cell_type":"markdown","source":["Install the OpenAI gym package to get the FrozenLake environment"]},{"metadata":{"id":"ghZqiQrYJrQj","colab_type":"code","outputId":"56eb0761-5238-4891-d1a3-c8e2762ccb42","executionInfo":{"status":"ok","timestamp":1544643479316,"user_tz":300,"elapsed":3070,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":210}},"cell_type":"code","source":["!pip install gym"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"],"name":"stdout"}]},{"metadata":{"id":"HZs11OeBtCls","colab_type":"text"},"cell_type":"markdown","source":["Import the following packages as wel'll need it later"]},{"metadata":{"id":"r9tegFLYOBuw","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import gym\n","import random\n","# For the plotting\n","from IPython.display import clear_output\n","from time import sleep"],"execution_count":0,"outputs":[]},{"metadata":{"id":"85XwN2dmOBuz","colab_type":"text"},"cell_type":"markdown","source":["Load the FrozenLake Environment"]},{"metadata":{"id":"vICyXX0tOBu2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"bf7dafd9-43cb-4ec1-8917-fba1de6a0409","executionInfo":{"status":"ok","timestamp":1544643479324,"user_tz":300,"elapsed":3047,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}}},"cell_type":"code","source":["env = gym.make(\"FrozenLake-v0\").env"],"execution_count":35,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"],"name":"stderr"}]},{"metadata":{"id":"_KoesXc6tMdB","colab_type":"text"},"cell_type":"markdown","source":["For the curious, we'll display the state and action space size for the game"]},{"metadata":{"id":"fLgr9L6CKYWL","colab_type":"code","outputId":"64f97ddf-9911-4636-a3a9-4f9ab39a47fa","executionInfo":{"status":"ok","timestamp":1544643479324,"user_tz":300,"elapsed":3033,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Action Space Discrete(4)\n","State Space Discrete(16)\n"],"name":"stdout"}]},{"metadata":{"id":"CrT7_vvgJj4e","colab_type":"text"},"cell_type":"markdown","source":["This is the graphical representation of the game\n","  - The highlight is your character\n","  - The `F` signifies a frozen spot\n","  - The `H` signifies a hole\n","  - `G` is the goal.\n","  \n","\n","The goal of this game is to navigate from your starting position to the goal without falling into any other holes. The catch is that this environment is stochastic, meaning that the direction you choose to go doesn't always happen. Sometimes you slip..."]},{"metadata":{"id":"OopZ1NqgrqD0","colab_type":"code","outputId":"91634234-0263-4f4d-9730-ba74759b245a","executionInfo":{"status":"ok","timestamp":1544643479326,"user_tz":300,"elapsed":3024,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"cell_type":"code","source":["env.render()"],"execution_count":37,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n"],"name":"stdout"}]},{"metadata":{"id":"VMr7pphvtSVH","colab_type":"text"},"cell_type":"markdown","source":["## Q Learning Agent"]},{"metadata":{"id":"2Ar3yLxrtU7P","colab_type":"text"},"cell_type":"markdown","source":["The QAgent class is going to house all the logic necessary to have a Q-Learning Agent. Since there's a lot going on here, this section will be longer than the others."]},{"metadata":{"id":"FNyEJDty0Vn_","colab_type":"text"},"cell_type":"markdown","source":["### Parameters"]},{"metadata":{"id":"kGIzk0E00XEh","colab_type":"text"},"cell_type":"markdown","source":["There are several parameters that are hard-coded into the model that should be tweaked when applying it to different problems to see if it affects performance. We will describe each parameter briefly here.\n","\n","\n","\n","* Epsilon: The exploration rate. How often will the agent choose a random move during training instead of relying on information it already has. Helps the agent go down paths it normally wouldn't in hopes for higher long term rewards.\n","  - Epsilon Decay: How much our epsilon decreases after each update\n","  - Epsilon Min: The lowest rate of exploration we'll allow during training\n","* Gamma: Discount rate. This tells us how much we prioritize current versus future rewards.\n","* Alpha: Affects how much we shift our knowledge based off of new information.\n","\n"]},{"metadata":{"id":"0TOKiRNm13E3","colab_type":"text"},"cell_type":"markdown","source":["### Fixed Q-Targets"]},{"metadata":{"id":"jGcn7vPp15VD","colab_type":"text"},"cell_type":"markdown","source":["In Q-Learning we update our Q_Table through the following function:\n","\n","$Q_{TableEntry}(state, action) = Reward + max(Q_{TableEntry}(state))$\n","\n","Since our update is dependent on the same table itself, we can start to get correlated entries. This could cause oscillations to happen in our training. \n","\n","To combat this, we implemented a target model. It essentially is a copy of the original model, except that the values do not update as rapidly. The rate at which the target model updates is dependent upon `Alpha` in our parameter list."]},{"metadata":{"id":"iqJQvpl94h5_","colab_type":"text"},"cell_type":"markdown","source":["### Agent Workflow"]},{"metadata":{"id":"HFW8vNbp5f-V","colab_type":"text"},"cell_type":"markdown","source":["1. Create an empty q-table for both the model and target model.\n","2. Given a starting state, perform an action.\n","3. Once you performed the action on the environment, update the model with information you have gained through the environment: reward, next state, if the environment is finished, etc.\n","4. Then calculate the value of the state.\n","  - If the game is finished, then the reward is the value of that state.\n","  - If not, then take the current reward and add the discounted reward of future states.\n","5. Update the model with the new value.\n","6. Decay the epsilon value as described in the parameters.\n","7. Gradually update the target model.\n","8. Perform an action for the new state, either randomly through epsilon, or by choosing the best action based on what we currently know.\n","9. Repeat steps 3-8."]},{"metadata":{"id":"lBz-t2khgRrs","colab_type":"code","colab":{}},"cell_type":"code","source":["class QAgent:\n","  def __init__(self, state_size, action_size):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.gamma = 0.6 # Discount Rate\n","    self.epsilon = 0.1 # Exploration Rate\n","    self.epsilon_min = 0.001\n","    self.epsilon_decay = 0.9995\n","    self.model = self._build_model()\n","    ## Additional components for Fixed Q-Targets\n","    self.target_model = self._build_model()\n","    # Update the target model by 10% each iteration\n","    self.alpha = 0.2\n","    \n","  def _build_model(self):\n","    # Assumes both self.state_size and self.action_size are lists\n","    model = np.zeros(self.state_size + self.action_size)\n","    return model\n","  \n","  def update_target_model(self):\n","    self.target_model = (1 - self.alpha) * self.target_model + self.alpha * self.model\n","    \n","  def act_random(self):\n","    return random.randrange(self.action_size[0])\n","  \n","  def best_act(self, state):\n","    # Choose the best action based on what we know\n","    # If all the action values are the same, then choose randomly\n","    action = self.act_random() if np.all(self.target_model[state, 0] == self.target_model[state]) else np.argmax(self.target_model[state])\n","    return action\n","  \n","  def act(self, state):\n","    # Act randomly epsilon percent of time, otherwise act greedily\n","    action = self.act_random() if np.random.rand() <= self.epsilon else self.best_act(state)\n","    return action\n","  \n","  def update(self, state, action, reward, next_state, done):\n","    target = reward\n","    if not done:\n","      target = reward + self.gamma * np.amax(self.target_model[next_state])\n","    self.model[state,action] = target\n","    self.update_target_model()\n","    if self.epsilon > self.epsilon_min:\n","      self.epsilon *= self.epsilon_decay\n","      \n","  \n","  def load(self, name):\n","    self.model.load_weights(name)\n","    \n","  def save(self, name):\n","    self.model.save_weights(name)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cFG8QUy0ta21","colab_type":"text"},"cell_type":"markdown","source":["## Training"]},{"metadata":{"id":"rMTb3bnLtc20","colab_type":"text"},"cell_type":"markdown","source":["Now that we have defined our agent, let us train it through playing a lot of games. By the end of it, we would hope that the agent has been through a variety of situations and have learned the best way to combat each one."]},{"metadata":{"id":"Ypz6n0kPrqES","colab_type":"code","outputId":"96011f93-056f-41ab-c791-c58b65bec774","executionInfo":{"status":"ok","timestamp":1544643656023,"user_tz":300,"elapsed":143,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"cell_type":"code","source":["%%time\n","\"\"\"Training the agent\"\"\"\n","state_size =  [ env.observation_space.n ]\n","action_size = [ env.action_space.n ]\n","agent = QAgent(state_size, action_size)\n","EPISODES = 100001\n","for i in range(1, EPISODES):\n","  state = env.reset()\n","  done = False\n","  while not done:\n","    action = agent.act(state)\n","    next_state, reward, done, info = env.step(action) \n","    agent.update(state, action, reward, next_state, done)\n","    state = next_state\n","       \n","  if i % 100 == 0:\n","    clear_output(wait=True)\n","    print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Episode: 100000\n","Training finished.\n","\n","CPU times: user 2min 55s, sys: 2.96 s, total: 2min 58s\n","Wall time: 2min 56s\n"],"name":"stdout"}]},{"metadata":{"id":"BbU--I3IrqEU","colab_type":"text"},"cell_type":"markdown","source":["## Evaluate Performance of agent"]},{"metadata":{"id":"JWI7dw4TrqED","colab_type":"text"},"cell_type":"markdown","source":["The code below animates the attempts"]},{"metadata":{"id":"AdD3ZUWqrqEE","colab_type":"code","colab":{}},"cell_type":"code","source":["def print_frames(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'].getvalue())\n","        print(f\"Timestep: {i + 1}\")\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        sleep(.2)\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"HpuNQMJ-LKAB","colab_type":"text"},"cell_type":"markdown","source":["Now onto the simulated trials."]},{"metadata":{"colab_type":"code","outputId":"2e79c6f9-5b93-4052-81ba-88d4e67ccbac","id":"JRMY1f2_Qb-T","colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"status":"ok","timestamp":1544643656029,"user_tz":300,"elapsed":179670,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}}},"cell_type":"code","source":["env.reset()\n","\n","\n","for episode in range(5):\n","    state = env.reset()\n","    frames = []\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"EPISODE \", episode)\n","\n","    while not done:\n","      step = step + 1\n","      action = agent.best_act(state)\n","       \n","      new_state, reward, done, info = env.step(action)\n","        \n","      frames.append({\n","        'frame': env.render(mode='ansi'),\n","        'state': state,\n","        'action': action,\n","        'reward': reward\n","      })\n","        \n","      if done:\n","        # Show the last state\n","        env.render()\n","\n","        # We print the number of step it took.\n","        print(\"Number of steps\", step)\n","        break\n","      state = new_state\n"],"execution_count":41,"outputs":[{"output_type":"stream","text":["****************************************************\n","EPISODE  0\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","Number of steps 124\n","****************************************************\n","EPISODE  1\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","Number of steps 48\n","****************************************************\n","EPISODE  2\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","Number of steps 52\n","****************************************************\n","EPISODE  3\n","  (Right)\n","SFFF\n","FHF\u001b[41mH\u001b[0m\n","FFFH\n","HFFG\n","Number of steps 33\n","****************************************************\n","EPISODE  4\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","Number of steps 21\n"],"name":"stdout"}]},{"metadata":{"id":"0R7hAvDsrqEa","colab_type":"code","outputId":"45182386-7ded-4c14-c060-84feffab5dfa","executionInfo":{"status":"ok","timestamp":1544643660332,"user_tz":300,"elapsed":118,"user":{"displayName":"Brandon R","photoUrl":"https://lh6.googleusercontent.com/-qrof4DnIwWE/AAAAAAAAAAI/AAAAAAAAAy8/40almYHDmLY/s64/photo.jpg","userId":"12615216155072347092"}},"colab":{"base_uri":"https://localhost:8080/","height":193}},"cell_type":"code","source":["print_frames(frames)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","\n","Timestep: 21\n","State: 14\n","Action: 1\n","Reward: 1.0\n"],"name":"stdout"}]},{"metadata":{"id":"69xHEEW8Sdl_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}